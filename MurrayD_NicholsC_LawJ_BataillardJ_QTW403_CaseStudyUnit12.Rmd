---
title: 'Branching Processes: A Simulation Study of Parameter Effects'
author: "Dennis Murray, Jared Law, Julien Bataillard, Cory Nichols"
date: "April 3rd, 2018"
output:
  word_document:
    fig_caption: yes
  pdf_document: default
section: MSDS 7333-403 - Quantifying the World - Case Study 6 (Unit 12)
---

```{r load_libs, echo=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(magrittr)
library(knitr)
library(scatterplot3d)
library(doParallel)
```

```{r setup, echo=FALSE, include=FALSE}
dir <- "~/DataScience/SMU/QTW/Unit12/CaseStudy/"
setwd(dir)
knitr::opts_knit$set(root.dir = dir)
knitr::opts_chunk$set(echo = FALSE)
```

```{r make, include=FALSE, echo=FALSE}
# load relevant functions for simulation
source("src/make.R")
```

## Abstract

Monte Carlo simulation is an extremely useful method to analyze complex mathematical relationships. When used to investigate a stochastic process, Monte Carlo simulation allows a researcher to analyze thousands of independent random outcomes given underlying distributional assumptions. In this paper, we model computer job queueing as a Poisson process. We use a Poisson distribution to estimate the number of jobs and an exponential distribution to estimate the lifetime of each respective job. We investigate the number of jobs created given these assumptions by simulating outcomes using different values of the Poisson lambda and exponential rate parameters. We find that the ratio of the two distribution parameters is more important than two discrete values in determining the number of jobs created. Thus, simulation parameters can be simplified to use only the ratio of the two parameters instead of iterating over hundreds of combinations.


## Introduction

In the context of software engineering, it is good practice to anticipate the likelihood of a release date, the cost of a project, or the impact of implementing new code base. These types of exercises usually fall into the category of risk analysis. However, it is often extremely difficult to successfully identify or anticipate outcomes if those outcomes are considered mostly random. One such software engineering example proudcing random outcomes is computer job queueing. 

Even with parallel processing, a queue of interdependent tasks can form, often slowing down processing time (Nolan, et. al 1). The number of these tasks can be completely random, specifically when considering the amount and type of requests the computer must consider. Typically, a parent job generates children which then generate additional children recursively. Each one of these jobs have a distinct run time that begins after the completion of their respective parent job. Obviously, this queueing process can result in significant slow down times, therefore it is often useful to simulate the outcomes of this branching process to optimize code and job processing logic.

In order to model the queueing process, we must utilize probability distributions to estimate the number of jobs created and their associated lifetimes. Given the randomness of the queueing process, we explore many different outcomes utilizing these probability distributions via Monte Carlo simulation. These simulations allow us to explore the distributions of job counts our queue model produces.

Of particular importance are the associated distributional parameters that must be identified as part of our simulation analysis. Our objective in this paper is to determine if these parameters and their values are related to the longevity of the queueing process. Specifically, we investigate whether discrete parameter values or the ratio of the two parameters is more important to the longevity of the queueing process.

In the following section, we review academic literature on stochastic processes and Monte Carlo simulation. In the methods section, we provide a description of the probability distribution assumptions used to model the CPU queueing process. We also identify the parameter selections and logic used to test the sensitivity of our model given the distributional parameters. Results of Monte Carlo simulation for the queueing process and associated parameters are then presented and compared. We conclude this paper by summarizing results and considering future work.

## Background 

Gabbiani and Cox (2) define a stochastic process as a collection of random variables indexed by a variable t, usually representing time. These processes are grouped into two categories: discrete-time and continuous-time stochastic processes. They are studied intensely as part of mathematical models to describe systems that occur in a random manner. The applications of these models can be found in computer science, cryptography, telecommunication, finance and many other academic disciplines and industries.

One of the easiest stochastic processes to understand is the Bernoulli process (3) as it only has two possible outcomes: success or failure, given probabilities of {p, 1-p}. For example, tossing a fair coin ten times can be modeled as a Bernoulli process with a sequence of ten coin-flip results and probabilities of 0.5 for head and 0.5 for tail. Each coin-flip is a binary-valued random variable. 

Another example of a stochastic process is the Poisson process which we use in this paper to estimate the number and run time of jobs in a queueing process. It is used in instances when we need to count the number of occurrences of an event that happens at random. In practice, (Pishro-Nik, 4) the Poisson process is used to model the number of car accidents at a site, the location of users in a wireless network, the request for individual documents on a web server and the outbreak of wars. 

The Poisson process with rate $\lambda$ is defined as:
Let $\lambda$ > 0 be fixed. The counting process {N(t), t $\elementof$ [0,$\inf$)} is called a Poisson process with rate $\lambda$ if all the following conditions hold:
1.	*N*(0) = 0;
2.	*N*(t) has independent increments;
3.	the number of arrivals in any interval of length $\tau$>0 has *Poisson*($\lambda$$\tau$) distribution.

Modeling a random process given only a few samples of that respective process can lead to inconclusive or erroneous assumptions. In order to empirically analyze these random processes, Monte Carlo simulations are used frequently. This method is used to generate random variables for modeling risk or uncertainty of a system (5). Monte Carlo simulations have some key advantages over more deterministic analyses (6) including simulated outcomes and probabiities for each outcome. It is easy to generate graphical representations based on the results, and one can easily spot which output has the largest effect on the bottom line results. Monte Carlo simulations are used extensively to simulate outcomes, especially for electoral processes and in sports betting.

## Methods

We must identify a method to model the previously described queue forming process in order to analyze many random outcomes given a set of distribution parameters. Our model assumptions are as follows:

1. The queueing process begins with a single parent job
2. This parent (and all subsequent parents) have a randomly generated run time or "lifetime"
3. Each parent can randomly generate n amount of children jobs
4. All child jobs are "protected" and do not run until their parent job completes or "dies"
5. Each child can spawn n amount of child jobs
6. The process runs until no additional children are generated or infinitely

This model represents a branching process, the outputs of which are the number of generations and the number of offspring in a family tree of jobs. Starting with the parent, each job has a fixed lifetime that is determined randomly. Within this fixed interval of time, a parent can generate *n* amount of child jobs. Thus, we can use a Poisson process to model the counts of jobs over each fixed time interval. Specifically, we use the Poission distribution to randomly generate the number of children for each parent given a rate parameter. Finally, each job runs for a certain amount of time, defining the time boundaries between parent end and child start times. We can randomly generate these run times using a random variable from the exponential distribution to determine the lifetime of each job. What results is a family tree with distinct generations containing *n* number of children. We can subsequently count these generations and the number of jobs to determine the longevity of our queueing process.

The Poisson and exponential distributions allow us to parameterize our queueing process. Each distribution contains a rate parameter, which represent the number of events per unit time. Specifically, we use lambda and kappa for our rate designations for the Poisson and exponential distributions, respectively. These rate parameters can elongate or shorten the queueing process dramatically. Obviously, if a parent generates many child jobs, the probability of a process continuing is greater. Further, if a job has a longer run time, the probability of a process continuing is also greater. It is the interaction of these two parameters we are interested in analyzing. As an example, we randomly generate 5000 samples from a Poisson and exponential distribution with rate parameters of 0.25 and 1 below.

Figure 1: Rate Parameter Comparisons from 5000 Randomly Generated Variables
```{r dist_comparisons, echo=FALSE, include=TRUE, cache=TRUE, fig.height=6, fig.width=8.5}

par(mfrow=c(2,2))
set.seed(1)
hist(rpois(5000, 0.25), 
     main = 'Poisson Distribution: Lambda of 0.25', 
     xlab = "Children")
hist(rpois(5000,1), 
     main = 'Exponential Distribution: Lambda of 1', 
     xlab = "Children")
plot(density(rexp(5000, 0.25)),
     main = 'Exponential Distribution: Kappa of 0.25', 
     xlab = "Lifetime", 
     ylim=c(0,0.8),
     xlim=c(0,15))
plot(density(rexp(5000, 1)), 
     main = 'Exponential Distribution: Kappa of 1', 
     xlab = "Lifetime", 
     ylim=c(0,0.8),
     xlim=c(0,15))
par(mfrow=c(1,1))

```

As can be seen in Figure 1, each distribution's rate parameter can significantly change the outcome for the number of children and the lifetime for each job in our model. We seek to understand if the ratio of these parameters are of importance, or if one parameter dominates when considering the longevity of our queueing process.

In order to investigate the relationship between lambda and kappa in our queueing model, we carry out a Monte Carlo simulation study by fixing kappa, which represents the parameter for the lifetime of each job, to be one. We run 500 simulations to determine the number of child jobs created using each of ten increasing values of lambda, which represents the rate parameter for the number of offspring for each parent. We then run another set of simulations where kappa is not equal to one, however, the ratio of lambda to kappa is equal to one of the lambda values from the earlier simulation where kappa was equal to one. This allows us to analyze whether the relationship between these two parameters are important or if one or more parameters dominates the longevity of our queueing process.


## Results

In order to test the relationship between the kappa and lambda parameters, we identified two test cases. In the first case, we hold kappa constant at one and test a range of lambda values from zero to one. We then double the lambda and kappa rates in the second case and examine comparisons between the two tests cases using Monte Carlo simulation. In both cases, kappa, or the rate parameter for the exponential distribution, is held as a constant *c*. This allows us to test the ratio of the two parameters directly. Additionally, it also ensures the ratio value of *k*/*c* is equal to one of the values of lambda from the first test case.

### Test Case One Results

```{r test_case1, include=FALSE, echo=FALSE, cache=TRUE}
lambdas = c(seq(0.1, 1, by = 0.1))
kappas = c(1)
# matrix of parameter vals to pass to monte carlo
paramGrid = as.matrix(expand.grid(lambdas, kappas))

# Simulation for constant Kappa values == 1
# MCBA takes a parameter matrix to iterate over
# it also implements a repeat for monte carlo sims per
# parameter combination
# returns a matrix with cols == number of reps, with each
# column containing num gens and num off spring in rows


set.seed(1)
#mcOutputK1 = MCBA(params = paramGrid, 
#                     repeats = 500, # number of repeats
#                     mG = 20, # max number of generations
#                     mO = 1000) # maximum number of offpspring
                     
# save data for later use
#save(mcOutputK1, file = "data/mcOutputK1.rda")
load(file = "data/mcOutputK1.rda")

```

Utilizing methods described in the previous section, we simulate our queueing process using Poisson lambda values between zero and one and a constant exponential rate parameter kappa of one. 

Figure 2 clearly shows a low value of lambda results in smaller quantities of offspring when kappa, which controls the run time of each job, is held constant at one. This makes logical sense, as the lambda rate as part of the Poisson distribution controls the amount of offpsring each parent produces in our queueing process. As lambda rises, holding kappa constant, we notice that our queueing process runs up against the generational (20) and offspring boundaries (1000), indicating a longer queueing process.

```{r gg_up, include=FALSE, echo=FALSE, cache=TRUE}
# faceted viz for simulation results
str(mcOutputK1)

# pull together experiment results into df for ggplot viz purposes
AllExperiments<-data.frame()

totalMembers <- length(mcOutputK1)
member <- 1

# frame df to show faceted monte carlo results given kappa = 1 and adjusted lambda
while (member <= totalMembers) {
  tmp<-as.data.frame(t(as.data.frame(mcOutputK1[member])))
  colnames(tmp) <- c("Generations", "Offspring")  
  
  tmp %<>% mutate(lambda = lambdas[member], kappa = kappas)
  
  AllExperiments <- rbind(AllExperiments, tmp)
  
  #sentry
  member<- member +1 
}
```

```{r testcase1_plots, include=TRUE, echo=FALSE, cache=TRUE, fig.width=8.5, fig.height=5}
# plot faceted experiment results
AllExperiments %>% ggplot(aes(x=Offspring, Generations)) + 
  geom_point(alpha=0.2) + facet_wrap( ~ lambda, ncol=5) + 
  scale_x_log10(labels=scales::comma) + 
  geom_jitter() + 
  ggtitle("Figure 2: Kappa of 1, Lambda As Shown On Facet Label")

```

The effect tapers off for generations above lambda = 1.0, with fewer generations, but more offspring produced for values of lambda above 1.2. We can aggregate this effect by examining the log upper quartile value for each lambda and kappa parameter value in Figure 3.

```{r uq_viz_bar, echo=FALSE, include=TRUE, fig.width=8.5, fig.height=4}

# viz - upper quartile of complete times - this is probably overkill considering
# we can just use a bar or line given we dont have an increasing kappa....
logUQkids = sapply(mcOutputK1, function(x) 
               log(quantile(x[2, ], probs = 0.75), base = 10))

data.frame(lambdas, logUQkids) %>%
ggplot(aes(x=lambdas, y=logUQkids))+
  geom_col()+
  ggtitle("Figure 3: Upper Quartile Offspring for Kappa of 1, Lambda on X Axis")

```

A large jump in offspring is noticeable when lambda crosses the 0.75 threshold. Given the Poisson rate parameter lambda has a significant influence on the number of offspring, we could hypothesize that raising lambda would ultimately result in a longer queueing process in most scenarios. However, we do not consider kappa, or the rate parameter for the lifetime of each job, in our simulation. 

### Test Case Two Results

```{r test_case2, echo=FALSE, include=FALSE, cache=TRUE}
# setting simulation values for kappa and lambda, where kappa != to a constant of 1
# book calls for lambda / constant, so simply double to keep ratios the same, test to see if we get similar results empirically
# we must keep Kappa constant here but != 1, simply double, test to see if simulations stay roughly same
# must ensure that the l/k rate is similar to l from previous sim as well, doubling is easiest here.
# testing to see if the ratio keeps these simulations roughly constant or if they differ expect constant

lambdas2 = lambdas*2
kappas2 = kappas*2
paramGrid = as.matrix(expand.grid(lambdas2, kappas2))

set.seed(1)
# simulation with doubled params -- we keep this as a .rda file to avoid re-processing
#mcOutputK2 = MCBA(params = paramGrid, 
#                     repeats = 500, 
#                     mG = 20, 
#                     mO = 1000)

# save for later
#save(mcOutputK2, file = "data/mcOutputK2.rda")
load(file = "data/mcOutputK2.rda")

```

Previously, we noticed that a smaller value of kappa results in longer lifetimes for each job. However, we have just seen that increasing the lambda parameter controlling the number of offspring for each parent resulted in dramatic increases in processing time in our queueing simulations. We directly test whether one parameter has an effect on our queueing process, or if the ratio between kappa and lambda is more important.

We double the lambda and kappa rate parameters for our second test case. This allows us to maintain the ratio between lambda and kappa from the first scenario, but also test whether or not lambda has a more dramatic effect on our queueing process longevity. Additionally, each ratio is equal to a lambda value from the previous test, to ensure we address the parameter ratio impact on our queueing process appropriately.

We test kappa using a constant of two and let lambda range from 0.2 to two. We iterate over each parameter combination and simulate offspring and generations from our queueing process 500 times. The offspring and generations output can be seen in Figure 4.

```{r gg_up2, include=TRUE, echo=FALSE, cache=TRUE}

# facet grid for viz purposes showing results
ModifiedParameters<-data.frame()

totalMembers <- length(mcOutputK2)
member <- 1

while (member <= totalMembers) {
  tmp<-as.data.frame(t(as.data.frame(mcOutputK2[member])))
  colnames(tmp) <- c("Generations", "Offspring")  
  
  tmp %<>% mutate(lambda = lambdas2[member], kappa = kappas2)
  
  ModifiedParameters <- rbind(ModifiedParameters, tmp)
  
  member<- member +1 
}
```

```{r testcase2_plots, include=TRUE, echo=FALSE, cache=TRUE, fig.width=8.5, fig.height=4}
ModifiedParameters %>% ggplot(aes(x=Offspring, Generations)) + 
  geom_point(alpha=0.2) + facet_wrap( ~ lambda, ncol=5) + 
  scale_x_log10(labels=scales::comma) + 
  geom_jitter() +
  ggtitle("Figure 4: Kappa of 2, Lambda As Shown On Facet Label")
```

A comparison of the visualization of the two tests show nearly identical results for generations and offspring. This would lead us to conclude that the ratio of the two values, lambda and kappa, are the determinants of the number of generations for a given number of offspring. One variable does not dominate the results of our queueing process given our simulation exercise.  The process shows similar clusters produced - a number of examples of the product of the Monte Carlo process producing up to ten offspring, and a number of iterations on larger values of lambda producing more than 1,000 offspring. 

Additionally, the bar chart for the upper quartile of offspring produced for each simulated parameter combination result is nearly identical to test case one.

```{r uq_viz_bar2, echo=FALSE, include=TRUE, fig.width=8.5, fig.height=5}

# viz - upper quartile of complete times - this is probably overkill considering
# we can just use a bar or line given we dont have an increasing kappa....
logUQkids = sapply(mcOutputK2, function(x) 
               log(quantile(x[2, ], probs = 0.75), base = 10))

data.frame(lambdas2, logUQkids) %>%
ggplot(aes(x=lambdas2, y=logUQkids))+
  geom_col()+
  ggtitle("Figure 5: Upper Quartile Offspring for Kappa of 2, Lambda on X Axis")

```


## Conclusions and Future Work

After investigating two tests cases, we have empirically confirmed that the ratio of parameters for the queueing process drives the longevity of the process. This result is in line with expectations. The exponential distribution shows longer lifetimes for jobs when its rate parameter is small. The Poisson distribution produces more children for each parent when its parameter is larger. Thus, there is a balance between these two parameters that must be attained in order to lengthen the queueing process.

For a software engineer, these results could help optimize parallel processing and cpu load. For instance, code could be written to optimize application performance by offloading processes to underutilized cores if the probability of branching is high. We know a process is more likely to extend if a parent job produces more children or if each job tends to have a longer run time. These characteristics can be analyzed dynamically and sent to the cpu in a structured and efficient manner to optimize available processing cycles. Software engineers could easily apply sensitivity analysis using Monte Carlo simulation while analzying parameter variables for assumed distributions in order to determine worst and best case scenarios for application performance.

The use of Monte Carlo simulation has become an increasing part of the public lexicon in recent years. Many models that are visible to the public include at some level a Monte Carlo simulation.  These include election results modeling, as well as the outcomes of major sports leagues.  Fivethirtyeight.com's models for both the election, as well as for results of sports leagues, include Monte Carlo simulations (Boice 7).  The method has also been mentioned in the context of quantative finance, genetics and many other fields.

The pitfalls of Monte Carlo simulation may be somewhat obvious. It relies on an assumption that the future follows some probabalistic distribution, and that the future distribution is consistent with the past values of the variable. Like most statistically-based forecast methods, there's some expectation that major shifts in the underlying causes of change in the behavior of the variable will remain the same. In the example of a model for a Financial Market, while the major shifts and catastrophic market events are possible in the simulation, the actual probability of the event might differ from the *n*-standard deviations from the mean that would be expected.

In this case, we simulated a process of birth and death of jobs for a central processing unit, with dependencies between the start and finish. We attempted to understand the total time required for a queueing process.  In the context of the case, these events were referenced as jobs or processes that are broken up for parallel computing, and the process by which the later jobs must wait for the completion of earlier jobs.  This same method could be employed for the analysis of a manufacturing line.  In manufacturing, the Monte Carlo process can be used to simulate a process with interdependencies, and variability. These studies allow for a greater understanding of risks specific to a process or even a business model. These simulations can help to enhance enterprise resource planning, supply chain continuity and minimize financial losses (Heffernan 8).


## References
1. Nolan, D., Temple Lang, D. DATA SCIENCE IN R: a Case Studies Approach to Computational Reasoning and Problem Solving. CRC PRESS, 2017.

2. F. Gabbiani, S. Cox. "Mathematics for Neuroscientists"" (Second Edition). 2017.

3.“Bernoulli Distribution: Definition and Examples.” Statistics How To, 29 July 2016. www.statisticshowto.com/bernoulli-distribution/.

4. H. Pishro-Nik. "Basic Concepts of the Poisson Process." Probability Course. https://www.probabilitycourse.com/chapter11/11_1_2_basic_concepts_of_the_poisson_process.php

5. "Definition of Monte Carlo Simumation." The Economic Times. https://economictimes.indiatimes.com/definition/monte-carlo-simulation

6. "Monte Carlo Simulation." Palisade, 2018.
http://www.palisade.com/risk/monte_carlo_simulation.asp

7. Boice, J. "How Our Soccer Projections Work." FiveThirtyEight, 19 Jan 2017. https://fivethirtyeight.com/features/how-our-club-soccer-projections-work/

8. Heffernan, R. "Monte Carlo and Manufacturing." Quality Magazine, 1 April 2014.
https://www.qualitymag.com/articles/91769-monte-carlo-and-manufacturing


```{r book_code_and_viz_scratch,  eval=FALSE}

# basic simulation
trialKappas = c(0.1, 10, 0.1, 10)
trialLambdas = c(0.1, 0.1, 10, 10)
trialParams = matrix(c(trialLambdas, trialKappas), ncol = 2)
mcTrialOutput = MCBA(params = trialParams, 
                     repeats = 100, 
                     mG = 200, 
                     mO = 1000)

#save(mcTrialOutput, file = "mcTrialOutput.rda")

oldPar = par(mfrow = c(2, 2), mar = c(3,3,1,1))

# plot 4 scatters showing the generations and
# number of children / offspring given trialParams
# call invis here b/c output is null in console
invisible(mapply(function(oneSet, lambda, kappa) {
  plot(x = oneSet[2,], y = jitter(oneSet[1, ], 1), log = "x",
       ylim = c(1,20), xlim = c(1, 10^7), pch = 19, cex = 0.6,
       xlab = "num offspring", ylab = "jittered generations1")
  text(x = 50, y = 15, bquote(paste(lambda == .(lambda))) )
  text(x = 300, y = 15, bquote(paste(kappa == .(kappa))) )
  
  }, 
  mcTrialOutput, lambda = trialLambdas, kappa = trialKappas,
  SIMPLIFY = TRUE))

par(oldPar)

# create a param grid

lambdas = c(seq(0.1, 0.6, by = 0.1), seq(0.8, 2, by = 0.2), 
            seq(2.25, 3, by = 0.25))
kappas = c(lambdas, 3.25, 3.50, 3.75, 4.00, 4.50, 5.00)

# expand grid results in a df, cast as matrix
paramGrid = as.matrix(expand.grid(lambdas, kappas))

#registerDoParallel(cores=4)
#mcGrid = MCBA(params = paramGrid, # parameter matrix
#              repeats = 400, # 400 simulations of familyTree for each param combo
#              mG = 20, # max generations per tree
#              mO = 1000) # max offspring

save(mcGrid, file = "data/mcGridOutput2.rda")


# viz - upper quartile of complete times
logUQkids = sapply(mcGrid, function(x) 
               log(quantile(x[2, ], probs = 0.75), base = 10))

UQCut = cut(logUQkids, breaks = c(-0.1, 0.5, 2, max(logUQkids)) )
# get colors for each cut cat
color3 = c("#b3cde3aa", "#8856a7aa", "#810f7caa")
colors = color3[UQCut] # map colors to each upper quartile cut

# 3D plot upper quartiles
library(scatterplot3d)
sdp = scatterplot3d(x = paramGrid[ , 1], y = paramGrid[ , 2], 
                    z = logUQkids, pch = 15, color = colors,
                    xlab = "Lambda", ylab = "Kappa",
                    zlab = "Upper Quartile Offspring",
                    angle = 120, type="h")

legend("left", inset = .08, bty = "n", cex = 0.8,
       legend = c("[0, 0.5)", "[0.5, 2)", "[2, 5)"), 
       fill = color3)



# plot
oldPar  = par(mar = c(4.1, 4.1, 0.5, 0.5))


# what proportions of families for each parameter set hit limits
# of simulation? in this case 20 generations or 1000 offspring
# notice that low values of K (lifetimes) always result in max simulations
# low values of lambda quit out early
mcGridAlive = sapply(mcGrid, function(oneParamSet) {
  sum((oneParamSet[1,] == 20) | (oneParamSet[2,] > 1000)) / 
    length(oneParamSet[2,]) })

filled.contour(lambdas, kappas, 
               matrix(mcGridAlive, nrow = length(lambdas), 
                      ncol = length(kappas)), 
               xlab = "Lambda", ylab = "Kappa", 
               xlim = c(0.1, 3), ylim = c(0.1, 3.1)) 
title('Proportion of Generations > Limit')
par(oldPar)


# plot
oldPar  = par(mar = c(4.1, 4.1, 2, 1))


# proportion of simulations that have 20 or more offspring
mcGridProp20kids = sapply(mcGrid, function(oneParamSet) {
  sum(oneParamSet[2,] > 19) / length(oneParamSet[2,]) })

# matrix for z value in image() 
mcGridProp20kidsMat = matrix(mcGridProp20kids, 
                             nrow = length(lambdas), 
                             ncol = length(kappas))

# breaks and colors for the image map based on proportion cuts
breaks = c(0, 0.10, 0.2, 0.3, 0.5, 0.7, 0.9, 1)
colors = rev(rainbow(10))[-(1:3)]

image(lambdas, kappas, mcGridProp20kidsMat, col = colors,
      breaks = breaks, xlab = "Lambda", ylab = "Kappa", 
      xlim = c(0.05, 3.05), ylim = c(0.05, 3.05))

midBreaks = (breaks[ -8 ] + breaks[ -1 ]) / 2
legend(x = 0.1, y = 3.25, legend = midBreaks, fill = colors, 
       bty = "n", ncol = 7,  xpd = TRUE)

par(oldPar)

```
```{r extra_viz_scratch,  eval=FALSE}

set.seed(seed1)
tree = familyTree(lambda = 0.4, 
                  kappa = 1, 
                  maxGen = 10, 
                  maxOffspring = 100)

# 3209 offspring w/2616 offspring in last gen
length(tree)
sapply(tree, nrow)
sum(sapply(tree, nrow))


# viz gens
maxLife = max(sapply(tree, function(gen) max(gen$completes)))
numFamily = sum(sapply(tree, nrow))

plot(0,0, ylim = c(0.5, numFamily + 0.5), xlim = c(0, maxLife),
     xlab = "Time", ylab = "", type ="n",
     axes = FALSE)

box()
axis(1)
numGen = length(tree)
numKids = sapply(tree, nrow)
treeN = tree[ 2:(numGen + 1) ]
birthNum = c(0, cumsum(sapply(tree, nrow))[ -length(tree)])
axis(2, at = birthNum + 1, 
     labels = paste("Gen", 1:numGen), tick = FALSE, las = 1)

mapply(function(gen, nextGen, birthNum) {
  birthOrder = birthNum + (1:nrow(gen))
  segments(x0 = gen$births, x1 = gen$completes, y0 = birthOrder,
          lwd = 3, col = "grey")
  abline(h = 0.5 + max(birthOrder), lty = 2, col="black" )
  if (all(!is.na(nextGen$births)))
  points(x = nextGen$births, y = birthOrder[nextGen$parentID],
         pch = 4)
  },
  gen = tree, nextGen = treeN, birthNum = birthNum )

set.seed(seed2)
exptOne(1, 0.5, 100, 1000)

```
