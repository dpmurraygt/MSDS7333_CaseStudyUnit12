---
title: 'Branching Processes: Simulating Birth and Assassinations .... update this'
author: "Dennis Murray, Jared Law, Julien Bataillard, Cory Nichols"
date: "April 3rd, 2018"
output:
  pdf_document: default
  word_document:
    fig_caption: yes
section: MSDS 7333-403 - Quantifying the World - Case Study 6 (Unit 12)
---

```{r load_libs, echo=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(magrittr)
library(knitr)
library(scatterplot3d)
library(doParallel)
```

```{r setup, echo=FALSE, include=FALSE}
dir <- "~/DataScience/SMU/QTW/Unit12/CaseStudy/"
setwd(dir)
knitr::opts_knit$set(root.dir = dir)
knitr::opts_chunk$set(echo = FALSE)
```

```{r make, include=FALSE, cache=TRUE, eval=FALSE}
# get and clean data
#source("src/make.R")
```


## Abstract

Monte Carlo simulation is an extremely useful method to analyze complex mathematical relationships. When used to investigate a stochastic process, Monte Carlo simulation allows a researcher to analyze thousands of independent random outcomes given underlying distributional assumptions. In this paper, we model computer job queueing as a Poisson process. We use a Poisson distribution to estimate the number of jobs and an exponential distribution to estimate the lifetime of each respective job. We investigate the number of jobs created given these assumptions by simulating outcomes using different values of the Poisson lambda and exponential rate parameters. We find that the ratio of the two distribution parameters is more important than two discrete values in determining the number of jobs created. Thus, simulation parameters can be simplified to use only the ratio of the two parameters instead of iterating over hundreds of combinations.


## Introduction

In the context of software engineering, it is often good practice to anticipate the likelihood of a release date, the cost of a project, or the impact of implementing new code base. These types of exercises usually fall into the category of risk analysis. However, it is often extremely difficult to successfully identify or anticipate outcomes if those outcomes are considered mostly random. One such software engineering example proudcing random outcomes is computer job queueing. 

Even with parallel processing, a queue of interdependent tasks can form, often slowing down processing time. The number of these tasks can be completely random, specifically when considering the amount and type of requests the computer must consider. Typically, a parent job generates children which then generate more children. Additionally, each one of these child jobs have a particular lifetime or run time and can generate additional child jobs. Each job ultimately depends on the completion of prior tasks, causing a slow down in processing time. 

In order to model the queueing process we must utilize probability distributions to estimate the number of jobs created and their associated lifetimes. Given the randomness of the queueing process, we explore many different outcomes utilizing these probability distributions via Monte Carlo simulation. These simulations allow us to explore the distributions of job counts our queue model produces.

Of particular importance are the associated distributional parameters that must be identified as part of our simulation analysis. Our objective is to determine if these parameters and their values are related to the longevity of the queueing process. Specifically, we investigate whether discrete parameter values or the ratio of the two parameters is more important to the longevity of the queueing process.

In the following section, we review academic literature on stochastic processes and Monte Carlo simulation. In the methods section, we provide a description of the probability distribution assumptions used to model the CPU queueing process. We also identify the parameter selections and logic used to test the sensitivity of our model given the distributional parameters. Results of Monte Carlo simulation for the queueing process and associated parameters are then presented and compared. We conclude this paper by summarizing results and considering future work.


## Background 

Gabbiani and Cox [1] define a stochastic process as a collection or ensemble of random variables indexed by a variable t, usually representing time. They exist in two types - discrete-time and continuous-time stochastic processes. They are widely used as mathematical models to describe systems that occur in a random manner. The applications are vast in computer science, cryptography, telecommunication, and finance but also in biology, physics, chemistry, and neuroscience. 

One of the easiest stochastic processes to understand is the Bernoulli process [2] as it only has two possible outcomes - yes/no, success/failure, 1/0, blue/red, head/tail. with probabilities of {p, 1-p}. For example, tossing a fair coin ten times can be modeled as a Bernoulli process with a sequence of ten coin-flip results and probabilities of 0.5 for head and 0.5 for tail (since it is a fair coin). Each coin-flip is a binary-valued random variable. 

Another example of a stochastic process is the Poisson process which we use in this paper to estimate the number and length of the jobs in the queue. It is used in instances when we need to count the number of occurrences of an event that happens at random. In practice [3] the Poisson process is used to model the number of car accidents at a site, the location of users in a wireless network, the request for individual documents on a web server, the outbreak of wars . 

The Poisson process with rate ?? is defined as [3]:
Let ?? > 0 be fixed. The counting process {N(t), t ??? [0,???)} is called a Poisson process with rates ?? if all the following conditions hold:
1.	N(0) = 0;
2.	N(t) has independent increments;
3.	the number of arrivals in any interval of length ??>0 has Poisson(????) distribution.

Now, in order to estimate the number of jobs and their lengths of time we simulate different outcomes by using Monte Carlo simulations. These are used to generate random variables for modeling risk or uncertainty of a system [4]. Monte Carlo simulations have some key advantages over more deterministic analyses [5] including - the results land probabilities so they show you what could happen but also how likely it is to happen; it is easy to generate graphical representations based on the results; and one can easily spot which output has the largest effect on the bottom line results. 


[1] Fabrizio Gabbiani, Steven James Cox, in Mathematics for Neuroscientists (Second Edition), 2017
[2] http://www.statisticshowto.com/bernoulli-distribution/
[3] https://www.probabilitycourse.com/chapter11/11_1_2_basic_concepts_of_the_poisson_process.php
[4] https://economictimes.indiatimes.com/definition/monte-carlo-simulation
[5] http://www.palisade.com/risk/monte_carlo_simulation.asp



## Methods

In order to anticipate job counts for our queueing example, we need to identify a method to model the previously described queue forming process. Given each job has a fixed lifetime and we are interested in the number of child jobs each parent job generates, we use a Poisson process to count the number of jobs. Given we view each job over a fixed interval of time, we can use the Poission distribution to estimate the number of jobs each parent generates. Additionally, each job runs for a certain amount of time. We can generate these run times using a random variable from the exponential distribution to determine the lifetime of each job.

```{r poisson process for each job}
prob0 <- dpois(x = 0:max.col(mcGrid), lambda = 2.5)
prob1 <- dpois(x = 0:max.col(mcGrid1), lambda = 2.5)
```

We then identified several values for both lambda and kappa across both cases where kappa is kept constant at 1 and where kappa = c and does not equal 1.  These values are indicated in the table below.

We selected several values for lambda and kappa with matching ratios to determing first in the test case to see the overall patterns that would emerge from the changes, and then the simulation case, iterating over 40 times to provide a representative sample.  Our first attempt was to change the lambda while keeping kappa constant at 1 as shown below.

```{r kappa_constant_trial}
trialKappas = c(1, 1, 1, 1, 1, 1)
trialLambdas = c(0.1, 0.5, 1, 1.5, 2, 3)
trialParams = matrix(c(trialLambdas, trialKappas), ncol = 2)

# Trial Simulation for constant Kappa values
# MCBA takes a parameter matrix to iterate over
# it also implements a repeat for monte carlo sims per
# parameter combination
# returns a matrix with cols == number of reps, with each
# column containing num gens and num off spring in rows
mcTrialOutput = MCBA(params = trialParams, repeats = 20, 
                     mG = 200, mO = 100000)
                     
# save data for later use
save(mcTrialOutput, file = "mcTrialOutput1.rda")
```

The first sample simulation over 40 iterations for the requisite sample size was then completed.

```{r kappa_constant_full}
# create a param grid
lambdas = c(0.1, 0.5, 1, 1.5, 2, 3)
kappas = c(1, 1, 1, 1, 1, 1)

# expand grid results in a df, cast as matrix
paramGrid = as.matrix(expand.grid(lambdas, kappas))

pdf("BA_Scatterplot3Dkids_Kappa1.pdf", width = 7, height = 6)

# sample simulation iterated over 40 times with at at most 20 levels and 1000 children
mcGrid = MCBA(params = paramGrid, repeats = 40, mG = 20,
                mO = 1000)
                
# save results for later use
save(mcGrid1, file = "mcGridOutput1.rda")
```
From this, we then changed the values for kappa and lambda to reflect the ratios used in the first example while ensuring that kappa did not equal 1.

```{r kappa_not_trial}
# setting trial simulation values for kappa and lambda, where kappa != 1
trialKappas = c(5, 0.5, 0.3, 4, 0.5, 0.3)
trialLambdas = c(0.5, 0.25, 0.3, 6, 1, 1.5)
trialParams = matrix(c(trialLambdas, trialKappas), ncol = 2)

# trial simulation
mcTrialOutput = MCBA(params = trialParams, repeats = 20, 
                     mG = 200, mO = 100000)

# save for later
save(mcTrialOutput, file = "mcTrialOutput0.rda")
```

And then the full run of 40 iterations of the simulation for our sample size.

```{r kappa_not_full}
# create a param grid
lambdas = c(seq(0.1, 0.6, by = 0.1), seq(0.8, 2, by = 0.2), 
            seq(2.25, 3, by = 0.25))
kappas = c(seq(0.1, 0.9, by = 0.1), seq(1.2, 2, by = 0.2),
           seq(2.25, 5, by = 0.25))
           
# expand grid results in a df, cast as matrix
paramGrid = as.matrix(expand.grid(lambdas, kappas))

pdf("BA_Scatterplot3Dkids_KappaNot1.pdf", width = 7, height = 6)

# full sample simulation iterated over 40 times
mcGrid = MCBA(params = paramGrid, repeats = 40, mG = 20,
                mO = 1000)

# save results for later use
save(mcGrid, file = "mcGridOutput0.rda")
```

## Results



## Conclusions and Future Work

The use of simulation, and especially Monte Carlo simulation, has become an increasing part of the public lexicon in recent years.  Many models that are visible to the public include at some level a Monte Carlo simulation.  These include election results modeling, as well as the outcomes of major sports leagues.  Fivethirtyeight.com's models for both the election, as well as for results of sports leagues, include Monte Carlo simulations. [1]  The method has also been mentioned in the context of quantative finance, genetics and many other fields.

The pitfalls of Monte Carlo simulation may be somewhat obvious.  It relies on an assumption that the future follows some probabalistic distribution, and that the future distribution is consistent with the past values of the variable.  Like most statistically-based forecast methods, there's some expectation that major shifts in the underlying causes of change in the behavior of the variable will remain the same.  In the example of a model for a Financial Market, while the major shifts and catastrophic market events are possible in the simulation, the actual probability of the event might differ from the *n*-standard deviations from the mean that would be expected.

In this case, we simulated a process of birth and death of jobs, with dependencies between the start and finish, to understand the total time required.  In the context of the case, these events were referenced as jobs or processes that are broken up for parallel computing, and the process by which the later jobs must wait for the completion of earlier jobs.  This same method could be employed for the analysis of a manufacturing line.  In manufacturing, the Monte Carlo process can be used to simulate a process with interdependencies, and variability. 

The use of the simulation can be used to identified bottlenecks in the process.[3]



## References
[1] https://fivethirtyeight.com/features/how-our-club-soccer-projections-work/
[2] https://www.sciencedirect.com/science/article/pii/S0168160509006680
[3] https://www.qualitymag.com/articles/91769-monte-carlo-and-manufacturing

## APPENDIX - BOOK CODE
```{r book_code}

# only dropped in what we need for any of the questions, even some
# of this is a bit redundant

# vector generate children in BA process
genKidsV = function(bTimes, cTimes, parentID, lambda = 0.5, kappa = 0.3) {
  # Determine how many children each job has
  parentAge = cTimes - bTimes
  # get interarrival times (births)
  numKids = rpois(n = length(parentAge), lambda = lambda * parentAge)
  # if for some reason no children, return NULL
  if (sum(numKids) == 0) return(NULL)
  
  # Determine the birth times of the children  
  # use mapply because runif not vectorizable
  # pass in vectors of numKids, birthTimes and completeTimes
  kidStats = 
    mapply(function(n, min, max) {
      # get birth times for number of kids as part of uniform dist.
      # sort them ascending
      births = sort(runif(n, min, max))
      # get lifetimes as exponential
      runtimes = rexp(n, rate = kappa)
      # add it to parent's complete time (max)
      completes = rep(max, n) + runtimes
      # wrap it in a df
      data.frame(births, completes)
    },
           n = numKids , min = bTimes, max = cTimes, 
           SIMPLIFY = FALSE)
  
 # return IDs for parents, kids, births and completes
  return(data.frame(parentID = rep(parentID, numKids),
                    kidID = 1:sum(numKids), 
                    births = unlist(lapply(kidStats, "[[", "births")), 
                    completes = unlist(lapply(kidStats,"[[", "completes"))
                    ))
}

# use familyTree with maxGen and maxOffspring params
# this function grows a tree instead of applying directly
# to genKidsV
familyTree = function(lambda = 0.5, kappa = 0.3, 
                      maxGen = 10, maxOffspring = 1000) {

  # Return value - a list with 1 data frame per generation.
  allGens = vector(mode = "list", length = maxGen)
  
  # Generate root of the tree
  allGens[[1]] = data.frame(parentID = NA, kidID = 1, 
                            births = 0, # parent is born at 0
                            completes = rexp(1, rate = kappa)) # parent complete 
  
  # sentry for number of offspring
  currentNumOffspring = 0

  # Generate future generations, one at a time.
  for (i in 2:maxGen) {
    nextGen = genKidsV(bTimes = allGens[[ (i - 1) ]]$births, # pass vector of births
                       cTimes = allGens[[ (i - 1) ]]$completes, # pass vector of completes
                       parentID = allGens[[ (i - 1) ]]$kidID, # pass kidID from prev
                       lambda = lambda, kappa = kappa) # lambda and kappas for kids and lifetimes
    
    # if nextGen DF is null, return all before current
    if (is.null(nextGen)) return(allGens[ 1:(i - 1) ]) 
    # else append this generation to the list
    allGens[[ i ]] = nextGen
    # increment sentry var
    currentNumOffspring = currentNumOffspring + nrow(nextGen)
    # stopping condition for number of offspring
    if (currentNumOffspring > maxOffspring) 
      return(allGens[1:i])
  }  
  allGens
}


# iterates over each generation to get num rows
# takes length of entire df to agg num gens
exptOne = function(l, k, mG, mO){
  # Helper function to call familyTree
  # Returns - summary statistics for analysis,
  
  aTree = familyTree(lambda = l, kappa = k, maxGen = mG,
                      maxOffspring = mO)
  numGen = length(aTree)
  numJobs = sum(sapply(aTree, nrow))
  c(numGen, numJobs)
}



# MCBA takes a parameter matrix to iterate over
# it also implements a repeat for monte carlo sims per
# parameter combination
# returns a matrix with cols == number of reps, with each
# column containing num gens and num off spring in rows
MCBA = function(params, repeats = 5, mG = 10, mO = 1000){
  # params: matrix columns of lambda and kappa values
  # For each lambda and kappa pair, run "repeats" times
  
  n = nrow(params)
  mcResults = vector("list", length = n)
  
  for (i in 1:n) {
    cat("param set is ", i, "\n")
    mcResults[[i]] = replicate(repeats,
                               exptOne(l = params[i, 1],
                                       k = params[i, 2],
                                       mG = mG, 
                                       mO = mO))
  }
  mcResults
}


# basic simulation
trialKappas = c(0.1, 10, 0.1, 10)
trialLambdas = c(0.1, 0.1, 10, 10)
trialParams = matrix(c(trialLambdas, trialKappas), ncol = 2)
mcTrialOutput = MCBA(params = trialParams, repeats = 100, 
                     mG = 200, mO = 1000)

#save(mcTrialOutput, file = "mcTrialOutput.rda")

oldPar = par(mfrow = c(2, 2), mar = c(3,3,1,1))

# plot 4 scatters showing the generations and
# number of children / offspring given trialParams
# call invis here b/c output is null in console
invisible(mapply(function(oneSet, lambda, kappa) {
  plot(x = oneSet[2,], y = jitter(oneSet[1, ], 1), log = "x",
       ylim = c(1,20), xlim = c(1, 10^7), pch = 19, cex = 0.6,
       xlab = "num offspring", ylab = "jittered generations1")
  text(x = 50, y = 15, bquote(paste(lambda == .(lambda))) )
  text(x = 300, y = 15, bquote(paste(kappa == .(kappa))) )
  
  }, 
  mcTrialOutput, lambda = trialLambdas, kappa = trialKappas,
  SIMPLIFY = TRUE))

par(oldPar)

# create a param grid

lambdas = c(seq(0.1, 0.6, by = 0.1), seq(0.8, 2, by = 0.2), 
            seq(2.25, 3, by = 0.25))
kappas = c(lambdas, 3.25, 3.50, 3.75, 4.00, 4.50, 5.00)

# expand grid results in a df, cast as matrix
paramGrid = as.matrix(expand.grid(lambdas, kappas))

#registerDoParallel(cores=4)
#mcGrid = MCBA(params = paramGrid, # parameter matrix
#              repeats = 400, # 400 simulations of familyTree for each param combo
#              mG = 20, # max generations per tree
#              mO = 1000) # max offspring

save(mcGrid, file = "data/mcGridOutput2.rda")


# viz - upper quartile of complete times
logUQkids = sapply(mcGrid, function(x) 
               log(quantile(x[2, ], probs = 0.75), base = 10))

UQCut = cut(logUQkids, breaks = c(-0.1, 0.5, 2, max(logUQkids)) )
# get colors for each cut cat
color3 = c("#b3cde3aa", "#8856a7aa", "#810f7caa")
colors = color3[UQCut] # map colors to each upper quartile cut

# 3D plot upper quartiles
library(scatterplot3d)
sdp = scatterplot3d(x = paramGrid[ , 1], y = paramGrid[ , 2], 
                    z = logUQkids, pch = 15, color = colors,
                    xlab = "Lambda", ylab = "Kappa",
                    zlab = "Upper Quartile Offspring",
                    angle = 120, type="h")

legend("left", inset = .08, bty = "n", cex = 0.8,
       legend = c("[0, 0.5)", "[0.5, 2)", "[2, 5)"), 
       fill = color3)



# plot
oldPar  = par(mar = c(4.1, 4.1, 0.5, 0.5))


# what proportions of families for each parameter set hit limits
# of simulation? in this case 20 generations or 1000 offspring
# notice that low values of K (lifetimes) always result in max simulations
# low values of lambda quit out early
mcGridAlive = sapply(mcGrid, function(oneParamSet) {
  sum((oneParamSet[1,] == 20) | (oneParamSet[2,] > 1000)) / 
    length(oneParamSet[2,]) })

filled.contour(lambdas, kappas, 
               matrix(mcGridAlive, nrow = length(lambdas), 
                      ncol = length(kappas)), 
               xlab = "Lambda", ylab = "Kappa", 
               xlim = c(0.1, 3), ylim = c(0.1, 3.1)) 
title('Proportion of Generations > Limit')
par(oldPar)


# plot
oldPar  = par(mar = c(4.1, 4.1, 2, 1))


# proportion of simulations that have 20 or more offspring
mcGridProp20kids = sapply(mcGrid, function(oneParamSet) {
  sum(oneParamSet[2,] > 19) / length(oneParamSet[2,]) })

# matrix for z value in image() 
mcGridProp20kidsMat = matrix(mcGridProp20kids, 
                             nrow = length(lambdas), 
                             ncol = length(kappas))

# breaks and colors for the image map based on proportion cuts
breaks = c(0, 0.10, 0.2, 0.3, 0.5, 0.7, 0.9, 1)
colors = rev(rainbow(10))[-(1:3)]

image(lambdas, kappas, mcGridProp20kidsMat, col = colors,
      breaks = breaks, xlab = "Lambda", ylab = "Kappa", 
      xlim = c(0.05, 3.05), ylim = c(0.05, 3.05))

midBreaks = (breaks[ -8 ] + breaks[ -1 ]) / 2
legend(x = 0.1, y = 3.25, legend = midBreaks, fill = colors, 
       bty = "n", ncol = 7,  xpd = TRUE)

par(oldPar)

```
```{r extra_viz}


set.seed(seed1)
tree = familyTree(lambda = 0.4, 
                  kappa = 1, 
                  maxGen = 10, 
                  maxOffspring = 100)

# 3209 offspring w/2616 offspring in last gen
length(tree)
sapply(tree, nrow)
sum(sapply(tree, nrow))


# viz gens
maxLife = max(sapply(tree, function(gen) max(gen$completes)))
numFamily = sum(sapply(tree, nrow))

plot(0,0, ylim = c(0.5, numFamily + 0.5), xlim = c(0, maxLife),
     xlab = "Time", ylab = "", type ="n",
     axes = FALSE)

box()
axis(1)
numGen = length(tree)
numKids = sapply(tree, nrow)
treeN = tree[ 2:(numGen + 1) ]
birthNum = c(0, cumsum(sapply(tree, nrow))[ -length(tree)])
axis(2, at = birthNum + 1, 
     labels = paste("Gen", 1:numGen), tick = FALSE, las = 1)

mapply(function(gen, nextGen, birthNum) {
  birthOrder = birthNum + (1:nrow(gen))
  segments(x0 = gen$births, x1 = gen$completes, y0 = birthOrder,
          lwd = 3, col = "grey")
  abline(h = 0.5 + max(birthOrder), lty = 2, col="black" )
  if (all(!is.na(nextGen$births)))
  points(x = nextGen$births, y = birthOrder[nextGen$parentID],
         pch = 4)
  },
  gen = tree, nextGen = treeN, birthNum = birthNum )

set.seed(seed2)
exptOne(1, 0.5, 100, 1000)

```
